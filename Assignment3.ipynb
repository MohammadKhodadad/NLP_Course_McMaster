{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khoda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel,BertForSequenceClassification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the classificationd at from asnq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'what is the use of fn key in mac', 'sentence': 'It is typically found on laptops due to their keyboard size restrictions .', 'label': 0, 'sentence_in_long_answer': False, 'short_answer_in_sentence': False}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"asnq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels=dataset['train']['label']\n",
    "labels=pd.Series(labels)\n",
    "labels0=labels[labels==0]\n",
    "labels1=labels[labels==1]\n",
    "max_len=2048\n",
    "assert(len(labels)==len(labels0)+len(labels1))\n",
    "selected_labels_train=list(pd.concat([labels0.sample(max_len),labels1.sample(max_len)]).index)\n",
    "\n",
    "\n",
    "\n",
    "labels=dataset['validation']['label']\n",
    "labels=pd.Series(labels)\n",
    "labels0=labels[labels==0]\n",
    "labels1=labels[labels==1]\n",
    "max_len=256\n",
    "assert(len(labels)==len(labels0)+len(labels1))\n",
    "selected_labels_val=list(pd.concat([labels0.sample(max_len),labels1.sample(max_len)]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASNQDataset(Dataset):\n",
    "    def __init__(self, asnq_split):\n",
    "        self.data = asnq_split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the question and answers\n",
    "        question = self.data[idx]['question']\n",
    "        answer = self.data[idx]['sentence']\n",
    "        label= self.data[idx]['label']\n",
    "        \n",
    "        output = tokenizer(question + \" [SEP] \" + answer, \n",
    "                                           add_special_tokens=True,\n",
    "                                           truncation=True, \n",
    "                                           max_length=128,\n",
    "                                           padding='max_length',\n",
    "                                           return_tensors='pt')\n",
    "        output['label']=torch.tensor([label],dtype=torch.float32)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=Subset(ASNQDataset(dataset['train']),selected_labels_train)\n",
    "val_dataset=Subset(ASNQDataset(dataset['validation']),selected_labels_val)\n",
    "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    corrects=0\n",
    "    total=0\n",
    "    model.eval()\n",
    "    for batch in tqdm.tqdm(val_loader):\n",
    "        input_ids=batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask=batch['attention_mask'].squeeze(1).to(device)\n",
    "        label=batch['label'].to(device)\n",
    "        output=model(input_ids,attention_mask)\n",
    "        corrects+=((output.logits>0.5)*1.==label).sum()\n",
    "        total+=output.logits.shape[0]\n",
    "    return corrects/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            input_ids=batch['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask=batch['attention_mask'].squeeze(1).to(device)\n",
    "            label=batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        acc=evaluate()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(epoch,avg_loss,acc.item())\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:45<00:00,  2.82it/s]\n",
      "100%|██████████| 16/16 [00:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.18299589978414588 0.8359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [02:06<00:00,  1.01it/s]\n",
      "100%|██████████| 16/16 [00:05<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.09128859001793899 0.837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [02:09<00:00,  1.01s/it]\n",
      "100%|██████████| 16/16 [00:05<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.04259881856341963 0.853515625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04259881856341963"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 13.8k/13.8k [00:00<00:00, 13.9MB/s]\n",
      "Downloading data: 100%|██████████| 594k/594k [00:00<00:00, 1.62MB/s]\n",
      "Downloading data: 100%|██████████| 264k/264k [00:00<00:00, 1.50MB/s]\n",
      "Downloading data: 100%|██████████| 2.00M/2.00M [00:00<00:00, 6.66MB/s]\n",
      "Generating test split: 100%|██████████| 6165/6165 [00:00<00:00, 343868.56 examples/s]\n",
      "Generating validation split: 100%|██████████| 2733/2733 [00:00<00:00, 909836.72 examples/s]\n",
      "Generating train split: 100%|██████████| 20360/20360 [00:00<00:00, 1696519.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wiki_qa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 6165\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 2733\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 20360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels=dataset['train']['label']\n",
    "labels=pd.Series(labels)\n",
    "labels0=labels[labels==0]\n",
    "labels1=labels[labels==1]\n",
    "max_len=512\n",
    "assert(len(labels)==len(labels0)+len(labels1))\n",
    "selected_labels_train=list(pd.concat([labels0.sample(max_len),labels1.sample(max_len)]).index)\n",
    "\n",
    "\n",
    "\n",
    "labels=dataset['validation']['label']\n",
    "labels=pd.Series(labels)\n",
    "labels0=labels[labels==0]\n",
    "labels1=labels[labels==1]\n",
    "max_len=64\n",
    "assert(len(labels)==len(labels0)+len(labels1))\n",
    "selected_labels_val=list(pd.concat([labels0.sample(max_len),labels1.sample(max_len)]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, asnq_split):\n",
    "        self.data = asnq_split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the question and answers\n",
    "        question = self.data[idx]['question']\n",
    "        answer = self.data[idx]['answer']\n",
    "        label= self.data[idx]['label']\n",
    "        \n",
    "        output = tokenizer(question + \" [SEP] \" + answer, \n",
    "                                           add_special_tokens=True,\n",
    "                                           truncation=True, \n",
    "                                           max_length=128,\n",
    "                                           padding='max_length',\n",
    "                                           return_tensors='pt')\n",
    "        output['label']=torch.tensor([label],dtype=torch.float32)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=Subset(WikiDataset(dataset['train']),selected_labels_train)\n",
    "val_dataset=Subset(WikiDataset(dataset['validation']),selected_labels_val)\n",
    "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:21<00:00,  1.46it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1914022695273161 0.7265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:20<00:00,  1.54it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.10124774661380798 0.6953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:21<00:00,  1.49it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.04649127341690473 0.7265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04649127341690473"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
