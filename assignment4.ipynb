{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset,Subset,DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import math,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"flytech/python-codes-25k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "separator_token = \"<|sep|>\"\n",
    "tokenizer.add_tokens([separator_token])\n",
    "tokenizer.pad_token=\"<|endoftext|>\"\n",
    "tokenizer.sep_token=\"<|sep|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens:\n",
      "Pad Token: <|endoftext|>\n",
      "EOS Token: <|endoftext|>\n",
      "BOS Token: <|endoftext|>\n",
      "UNK Token: <|endoftext|>\n",
      "Sep Token: <|sep|>\n",
      "CLS Token: None\n",
      "Mask Token: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Special Tokens:\")\n",
    "print(\"Pad Token:\", tokenizer.pad_token)\n",
    "print(\"EOS Token:\", tokenizer.eos_token)\n",
    "print(\"BOS Token:\", tokenizer.bos_token)\n",
    "print(\"UNK Token:\", tokenizer.unk_token)\n",
    "print(\"Sep Token:\", tokenizer.sep_token)\n",
    "print(\"CLS Token:\", tokenizer.cls_token)\n",
    "print(\"Mask Token:\", tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n=1000\n",
    "test_n=200\n",
    "train_data=Subset(dataset['train'],[i for i in range(train_n)])\n",
    "test_data=Subset(dataset['train'],[i+train_n for i in range(test_n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_tokenize(examples):\n",
    "    concatenated = examples['instruction'] + \" <|sep|> \" + examples['input'] + \" <|sep|> \" + examples['output'] + \" <|endoftext|>\"\n",
    "    return tokenizer(concatenated, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens_dict=concatenate_and_tokenize(self.dataset[idx])\n",
    "        item = {key: torch.tensor(val).to(device) for key, val in tokens_dict.items()}\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "train_loader= DataLoader(train_dataset,batch_size=8,shuffle=True)\n",
    "test_loader= DataLoader(test_dataset,batch_size=8,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraConv1D(nn.Module):\n",
    "    def __init__(self, c_attn_layer, rank=10):\n",
    "        super().__init__()\n",
    "        self.c_attn = c_attn_layer\n",
    "        self.rank = rank\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(self.c_attn.weight.shape[0], rank))\n",
    "        self.B = nn.Parameter(torch.randn(rank, self.c_attn.weight.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.c_attn.nf,)\n",
    "        delta_W = self.A @ self.B\n",
    "        adapted_weight = self.c_attn.weight + delta_W\n",
    "        x = torch.addmm(self.c_attn.bias, x.view(-1, x.size(-1)), adapted_weight)\n",
    "        x = x.view(size_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "tensor(-0.1101)\n",
      "tensor(0.0514)\n",
      "torch.Size([50258, 768])\n",
      "tensor(-0.1101)\n",
      "tensor(0.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-9): 10 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): LoraConv1D(\n",
       "            (c_attn): Conv1D()\n",
       "          )\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "print(model.transformer.wte.weight.data.shape)\n",
    "print(model.transformer.wte.weight.data[0][0])\n",
    "print(model.transformer.wte.weight.data[-1][0])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "new_embedding_vector = torch.zeros((1, model.config.hidden_size))\n",
    "model.transformer.wte.weight.data[-1, :] = new_embedding_vector\n",
    "print(model.transformer.wte.weight.data.shape)\n",
    "print(model.transformer.wte.weight.data[0][0])\n",
    "print(model.transformer.wte.weight.data[-1][0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for index in [10,11]:\n",
    "    model.transformer.h[index].attn.c_attn = LoraConv1D(model.transformer.h[index].attn.c_attn, rank=10)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(model=model, test_loader=test_loader, tokenizer=tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * inputs[\"input_ids\"].size(1)\n",
    "            total_words += inputs[\"input_ids\"].size(1)\n",
    "\n",
    "    average_loss = total_loss / total_words\n",
    "    perplexity = math.exp(average_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149984020.1826658"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_perplexity(model,test_loader,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model=model,train_loader=train_loader,test_loader=test_loader,\n",
    "          tokenizer=tokenizer,optimizer=optimizer,loss_function=loss_function, \n",
    "          epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            outputs=model(**batch,labels=batch['input_ids'])\n",
    "            loss=outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        average_loss=total_loss/len(train_loader)\n",
    "        print(f\"EPOCH: {epoch}, loss: {average_loss}, test_perplexity: {evaluate_perplexity()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:07<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, loss: 19.478653106689453, test_perplexity: 48175164.87326563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:51<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, loss: 18.478799995422364, test_perplexity: 21615301.18786448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:41<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2, loss: 17.566429870605468, test_perplexity: 9585959.299039198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:44<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3, loss: 16.992934974670412, test_perplexity: 5429467.01775179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 104/125 [04:07<00:49,  2.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, tokenizer, optimizer, loss_function, epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 13\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m average_loss\u001b[38;5;241m=\u001b[39mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test_perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate_perplexity()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
